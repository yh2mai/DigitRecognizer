{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c84dece-ccd4-45d1-ab2d-d84be4cee6e4",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Recognizer using Deep CNN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22d64f71-1824-45c4-92ef-4cc0408199ff",
   "metadata": {},
   "source": [
    "Process flow:\n",
    "1. Importing libraries\n",
    "2. Preparing the Dataset\n",
    "3. Model Building\n",
    "4. Model Fitting\n",
    "5. Predicting the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7b583a-9bb4-45f8-a903-4d41e13a1e30",
   "metadata": {},
   "source": [
    "# 1. Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf52aad-b3c0-4eea-915a-137a5502240a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.8.0+cu128\n",
      "CUDA Available: True\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg  # similar to keras image reading\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Torch versions\n",
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822c85d-dc93-40ea-8d27-bb34bbd240ff",
   "metadata": {},
   "source": [
    "# 2. Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c5210-8ca5-4aad-8c5e-25a26fdfc11e",
   "metadata": {},
   "source": [
    "Load the data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a95107-64b8-4142-aeae-290892531bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46cb69e-a0b7-457f-ac68-b2fee7aa0a4c",
   "metadata": {},
   "source": [
    "Separate features and target values and convert to pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfd9293-468e-4d75-9043-db6926d2a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(df_train['label'].values)\n",
    "x_train = torch.tensor(df_train.drop(labels=['label'],axis=1).values)\n",
    "x_test = torch.tensor(df_test.values)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88bde79e-68f0-431f-96c1-09fa8a7d1bf1",
   "metadata": {},
   "source": [
    "# Check for any null or missing values, fill nan with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81006a49-592a-4842-b3b6-d4bb507664f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.nan_to_num(x_train, nan=0)\n",
    "x_test = torch.nan_to_num(x_test, nan=0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f4bf425-eb30-4152-b27b-9e29e5ceb563",
   "metadata": {},
   "source": [
    "Normalisation\n",
    "Normalisation is done to reduce the scale of the input values. The pixel values range from 0 to 255 which specify gradient of gray. The CNN will converge more faster on values 0 to 1 than 0 to 255. So we divide every value by 255 to scale the data from [0..255] to [0..1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b13c49-a1e8-46e8-b811-666dab696191",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5558542-5f34-404b-90ee-2852b8d4c28a",
   "metadata": {},
   "source": [
    "The array of values will be reshaped into a (28,28,1) matrix. We are feeding the CNN model with input_shape of 28x28x1 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101ffee4-00a6-445a-93b1-214d15fdfaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.view(-1,1,28,28)\n",
    "x_test = x_test.view(-1,1,28,28)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c61971ab-475f-4488-bf3b-04206816af5a",
   "metadata": {},
   "source": [
    "Split the training and cross validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac17246-2541-4fd1-8dd6-bb56492b55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=2\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.1, random_state=random_seed)\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=10)\n",
    "valid_ds = TensorDataset(x_cv, y_cv)\n",
    "valid_dl = DataLoader(valid_ds, batch_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce54922-f252-4b9b-bbea-c0f1820bf238",
   "metadata": {},
   "source": [
    "# 3. Model Building"
   ]
  },
  {
   "cell_type": "raw",
   "id": "111e3a4c-4408-4b73-8be1-180f3658f54b",
   "metadata": {},
   "source": [
    "We use LeNet-5 Architecture, it's pretty popular for its minimal structure and easy to train nature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e61e3c0a-bb7d-433e-ba39-a9bd5437418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (6): Flatten(start_dim=1, end_dim=-1)\n",
      "  (7): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# CNN Architecture is IN -> [[Conv2D -> relu] * 2 -> MaxPool2D -> Dropout] * 2 -> #Flatten -> Dense -> Dropout ->Out\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # C1: Conv layer\n",
    "    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1,padding=0),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # S2: Avg Pooling\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # C3: Conv layer\n",
    "    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1,padding=0),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # S4: Avg Pooling\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # Flatten\n",
    "    nn.Flatten(),\n",
    "\n",
    "    # Dense Layers\n",
    "    nn.Linear(16 * 4 * 4, 120),  # after 2 poolings: 28 -> 14 -> 7\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120,84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10)\n",
    ")\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe7ffb6-2f35-45c5-a190-8bb7193607e9",
   "metadata": {},
   "source": [
    "# 4. Model fitting"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fee0e9df-1bc8-4809-8fa9-37a356d031e3",
   "metadata": {},
   "source": [
    "Optimizers and Annealers\n",
    "Optimizer is very important in a neural network. Optimizers ensure the model converge to the optimal faster. RMSProp optimizer makes the model converge more effectively and faster. It also stricts the model to converge at a global minimum therefore the accuracy of the model will be higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35ac7928-6597-473f-8f9a-22b409a0590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy\n",
    "epoch = 10\n",
    "def preprocess(x, y):\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "    \n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer (RMSprop)\n",
    "optimizer = optim.RMSprop(\n",
    "    model.parameters(),  # trainable parameters of your model\n",
    "    lr=0.001,\n",
    "    alpha=0.9,\n",
    "    eps=1e-08\n",
    ")\n",
    "\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5976eded-cc96-425f-877e-0f886d6b0bd8",
   "metadata": {},
   "source": [
    "Model Fitting or Model Training is where we train our model and evaluate the error parameters. Training process typically take a lot of time when it runs in a CPU. But the training can be speeeded up with the graphics card that have CUDA support. Kaggle have inbuilt limited GPU Support be sure to turn it on when running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee2dd3-b790-4130-ab80-1402ffefe4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.12417017679489661\n",
      "1 0.08994748872750538\n",
      "2 0.10796662219838275\n",
      "3 0.09858448090999947\n",
      "4 0.11602350172746721\n",
      "5 0.10561334942956305\n",
      "6 0.11071384791142243\n",
      "7 0.07840433444547792\n",
      "8 0.10672140037368513\n",
      "9 0.07109496178111527\n"
     ]
    }
   ],
   "source": [
    "fit(epoch, model, loss_func, optimizer, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7972df68-f024-439f-94a6-7e78990e3e39",
   "metadata": {},
   "source": [
    "# 5. Predicting the test data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91f2df2e-b46b-447e-a299-ff65fe74d138",
   "metadata": {},
   "source": [
    "Finally, we are predicting the test dataset for the competition. We predict the test data and store it in a csv file for competition submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "379e792b-6c87-4da8-8eb2-7c69d9a2576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict result\n",
    "x_test = x_test.to(device)\n",
    "results = torch.argmax(model(x_test), dim=1)\n",
    "result_numpy = results.detach().cpu().numpy()\n",
    "df = pd.DataFrame(result_numpy)\n",
    "df.index = df.index + 1\n",
    "df.to_csv(\"submission.csv\", index_label=\"ImageId\", header = [\"Label\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
